{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7623f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23618d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ab170a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.3333333333333333, 0.3333333333333333)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Problem 3.1\n",
    "\n",
    "# Prior class probabilities with Laplace Smoothing\n",
    "logye, logyj, logys = np.log((10 + 1/2)/(30 + 3/2)), np.log((10 + 1/2)/(30 + 3/2)), np.log((10 + 1/2)/(30 + 3/2))\n",
    "\n",
    "np.exp(logye), np.exp(logyj), np.exp(logys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508118ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count for each letter in the document given by file with name fname\n",
    "def get_counts(fname):\n",
    "    x = np.zeros(27, dtype=int)\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            for ch in line.strip():\n",
    "                if ch == ' ':\n",
    "                    x[26] += 1\n",
    "                else:\n",
    "                    x[ord(ch)-ord('a')] += 1\n",
    "                    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df11e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 0-9 -> english, 10-19 -> japanese, 20 -> 29 -> spanish\n",
    "# Example: 15 -> j5.txt\n",
    "def get_filename(idx):\n",
    "    pfx = ['e', 'j', 's']\n",
    "    return f'../languageID/{pfx[idx//10]}{idx%10}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62f1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y -> {e: 0, j: 1, s: 2}\n",
    "# X -> character count for each document\n",
    "def get_train_data():\n",
    "    Y = np.array([i//10 for i in np.arange(30)])\n",
    "    X = np.array([get_counts(get_filename(idx)) for idx in np.arange(30)])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63582dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1660f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 3.2\n",
    "\n",
    "# idx determines the lang {e: 0, j: 1, s: 2}\n",
    "def calc_params(idx):\n",
    "    # Accumulate counts for language idx\n",
    "    p = X[idx*10:(idx+1)*10].sum(axis=0)\n",
    "\n",
    "    # Laplace Smoothing (and broadcasting)\n",
    "    p = p + 1/2\n",
    "\n",
    "    # Normalize\n",
    "    return p / p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bf525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.06024017, 0.01114824, 0.02153561, 0.02199874, 0.10549472,\n",
       "        0.01895531, 0.01749975, 0.04727249, 0.05547653, 0.00142248,\n",
       "        0.00373813, 0.02901188, 0.02054319, 0.05799067, 0.06454067,\n",
       "        0.01677197, 0.00056237, 0.05388865, 0.06626088, 0.08022098,\n",
       "        0.02669622, 0.00929571, 0.0155149 , 0.00115783, 0.01386086,\n",
       "        0.00062854, 0.17827252]),\n",
       " array([1.34175414e-01, 1.10656467e-02, 5.58619463e-03, 1.75413627e-02,\n",
       "        6.13058175e-02, 3.94947518e-03, 1.42679239e-02, 3.23429995e-02,\n",
       "        9.88080413e-02, 2.38391745e-03, 5.84593489e-02, 1.45881516e-03,\n",
       "        4.05265967e-02, 5.77477317e-02, 9.28304572e-02, 8.89521437e-04,\n",
       "        1.06742572e-04, 4.35865504e-02, 4.29460950e-02, 5.80323786e-02,\n",
       "        7.19089130e-02, 2.49066002e-04, 2.01031845e-02, 3.55808575e-05,\n",
       "        1.44102473e-02, 7.86336951e-03, 1.07418609e-01]),\n",
       " array([1.04831978e-01, 8.25424305e-03, 3.76232726e-02, 3.98491359e-02,\n",
       "        1.14106409e-01, 8.62522027e-03, 7.20314094e-03, 4.54447089e-03,\n",
       "        4.99891798e-02, 6.64667512e-03, 2.78232912e-04, 5.30806566e-02,\n",
       "        2.58756608e-02, 5.43172473e-02, 7.26806195e-02, 2.43299224e-02,\n",
       "        7.69777723e-03, 5.94490988e-02, 6.59412001e-02, 3.57065570e-02,\n",
       "        3.37898414e-02, 5.90472069e-03, 9.27443040e-05, 2.50409621e-03,\n",
       "        7.88326584e-03, 2.68958481e-03, 1.66105048e-01]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe, pj, ps = calc_params(0), calc_params(1), calc_params(2)\n",
    "\n",
    "# Conditional prob of each char given class label\n",
    "pe, pj, ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb298091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex output\n",
    "\n",
    "def latex_table(pl):\n",
    "    headers = ['character', '$p(c \\mid e)$']\n",
    "\n",
    "    textabular = '|| c | c ||'\n",
    "    texheader = \" & \".join(headers) + \"\\\\\\\\\"\n",
    "    texdata = \"\\\\hline\\\\hline\\n\"\n",
    "    for i, p in enumerate(p):\n",
    "        if i == 26:\n",
    "            c = 'SPACE'\n",
    "        else:\n",
    "            c = chr(ord('a') + i)\n",
    "        texdata += f'{c} & {p:.3f}\\\\\\\\\\n'\n",
    "        texdata += \"\\\\hline\\n\"\n",
    "\n",
    "    print(\"\\\\begin{tabular}{\"+textabular+\"}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(texheader)\n",
    "    print(texdata,end=\"\")\n",
    "    print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c239d8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|| c | c ||}\n",
      "\\hline\n",
      "character & $p(c \\mid e)$\\\\\n",
      "\\hline\\hline\n",
      "a & 0.105\\\\\n",
      "\\hline\n",
      "b & 0.008\\\\\n",
      "\\hline\n",
      "c & 0.038\\\\\n",
      "\\hline\n",
      "d & 0.040\\\\\n",
      "\\hline\n",
      "e & 0.114\\\\\n",
      "\\hline\n",
      "f & 0.009\\\\\n",
      "\\hline\n",
      "g & 0.007\\\\\n",
      "\\hline\n",
      "h & 0.005\\\\\n",
      "\\hline\n",
      "i & 0.050\\\\\n",
      "\\hline\n",
      "j & 0.007\\\\\n",
      "\\hline\n",
      "k & 0.000\\\\\n",
      "\\hline\n",
      "l & 0.053\\\\\n",
      "\\hline\n",
      "m & 0.026\\\\\n",
      "\\hline\n",
      "n & 0.054\\\\\n",
      "\\hline\n",
      "o & 0.073\\\\\n",
      "\\hline\n",
      "p & 0.024\\\\\n",
      "\\hline\n",
      "q & 0.008\\\\\n",
      "\\hline\n",
      "r & 0.059\\\\\n",
      "\\hline\n",
      "s & 0.066\\\\\n",
      "\\hline\n",
      "t & 0.036\\\\\n",
      "\\hline\n",
      "u & 0.034\\\\\n",
      "\\hline\n",
      "v & 0.006\\\\\n",
      "\\hline\n",
      "w & 0.000\\\\\n",
      "\\hline\n",
      "x & 0.003\\\\\n",
      "\\hline\n",
      "y & 0.008\\\\\n",
      "\\hline\n",
      "z & 0.003\\\\\n",
      "\\hline\n",
      "SPACE & 0.166\\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "### Problem 3.3\n",
    "\n",
    "latex_table(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86e0cc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([164,  32,  53,  57, 311,  55,  51, 140, 140,   3,   6,  85,  64,\n",
       "       139, 182,  53,   3, 141, 186, 225,  65,  31,  47,   4,  38,   2,\n",
       "       492])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Problem 3.4\n",
    "\n",
    "xtest = get_counts(f'../languageID/e10.txt')\n",
    "\n",
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98890cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|| c | c ||}\n",
      "\\hline\n",
      "character & count\\\\\n",
      "\\hline\\hline\n",
      "a & 164\\\\\n",
      "\\hline\n",
      "b & 32\\\\\n",
      "\\hline\n",
      "c & 53\\\\\n",
      "\\hline\n",
      "d & 57\\\\\n",
      "\\hline\n",
      "e & 311\\\\\n",
      "\\hline\n",
      "f & 55\\\\\n",
      "\\hline\n",
      "g & 51\\\\\n",
      "\\hline\n",
      "h & 140\\\\\n",
      "\\hline\n",
      "i & 140\\\\\n",
      "\\hline\n",
      "j & 3\\\\\n",
      "\\hline\n",
      "k & 6\\\\\n",
      "\\hline\n",
      "l & 85\\\\\n",
      "\\hline\n",
      "m & 64\\\\\n",
      "\\hline\n",
      "n & 139\\\\\n",
      "\\hline\n",
      "o & 182\\\\\n",
      "\\hline\n",
      "p & 53\\\\\n",
      "\\hline\n",
      "q & 3\\\\\n",
      "\\hline\n",
      "r & 141\\\\\n",
      "\\hline\n",
      "s & 186\\\\\n",
      "\\hline\n",
      "t & 225\\\\\n",
      "\\hline\n",
      "u & 65\\\\\n",
      "\\hline\n",
      "v & 31\\\\\n",
      "\\hline\n",
      "w & 47\\\\\n",
      "\\hline\n",
      "x & 4\\\\\n",
      "\\hline\n",
      "y & 38\\\\\n",
      "\\hline\n",
      "z & 2\\\\\n",
      "\\hline\n",
      "SPACE & 492\\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Latex output\n",
    "\n",
    "headers = ['character', 'count']\n",
    "\n",
    "textabular = '|| c | c ||'\n",
    "texheader = \" & \".join(headers) + \"\\\\\\\\\"\n",
    "texdata = \"\\\\hline\\\\hline\\n\"\n",
    "for i, p in enumerate(xtest):\n",
    "    if i == 26:\n",
    "        c = 'SPACE'\n",
    "    else:\n",
    "        c = chr(ord('a') + i)\n",
    "    texdata += f'{c} & {p:}\\\\\\\\\\n'\n",
    "    texdata += \"\\\\hline\\n\"\n",
    "\n",
    "print(\"\\\\begin{tabular}{\"+textabular+\"}\")\n",
    "print(\"\\\\hline\")\n",
    "print(texheader)\n",
    "print(texdata,end=\"\")\n",
    "print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "748deeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7831.531702560013, -8786.051104707092, -8457.039705604957)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Problem 3.5\n",
    "\n",
    "xtest.T @ np.log(pe), xtest.T @ np.log(pj), xtest.T @ np.log(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "229422ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 0, 10,  0],\n",
       "       [ 0,  0, 10]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Problem 3.7\n",
    "\n",
    "cm = np.zeros((3, 3), dtype=int)\n",
    "labels = ['e', 'j', 's']\n",
    "\n",
    "for label in range(3):\n",
    "    for idx in range(10):\n",
    "        fname = f'../languageID/{labels[label]}{idx}.txt'\n",
    "        xtest = get_counts(fname)\n",
    "        likelihood = np.array([xtest.T @ np.log(pe) + logye, xtest.T @ np.log(pj) + logyj, xtest.T @ np.log(ps) + logys])\n",
    "        pred = np.argmax(likelihood)\n",
    "        cm[pred, label] += 1\n",
    "        \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c591de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> flatten -> h1 -> out -> softmax -> cross entropy loss\n",
    "\n",
    "d = 28 * 28 # Input Layer\n",
    "d1 = 512    # Hidden Layer\n",
    "k = 10      # Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8414f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 64\n",
    "X = torch.rand(N, 28, 28)\n",
    "Y = torch.randint(0, k, (N,))\n",
    "w1 = torch.empty((d1, d)) # Hidden layer\n",
    "b1 = torch.empty(d1)\n",
    "nn.init.kaiming_uniform_(w1, a=math.sqrt(5))\n",
    "bound = 1 / math.sqrt(d)\n",
    "nn.init.uniform_(b1, -bound, bound)\n",
    "w2 = torch.empty((k, d1)) # Output layer\n",
    "b2 = torch.empty(k)\n",
    "nn.init.kaiming_uniform_(w2, a=math.sqrt(5))\n",
    "bound = 1 / math.sqrt(d1)\n",
    "nn.init.uniform_(b2, -bound, bound)\n",
    "\n",
    "w1.shape, b1.shape, w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "\n",
    "# Flatten the input\n",
    "# flat.shape = (N, d)\n",
    "flat = X.view(N, -1)\n",
    "\n",
    "# First hidden layer\n",
    "lin1 = flat @ w1.T + b1\n",
    "h1 = torch.maximum(lin1, torch.tensor(0)) # ReLU. Let broadcast do its work\n",
    "\n",
    "# Output layer\n",
    "out = h1 @ w2.T + b2\n",
    "\n",
    "# Softmax\n",
    "maxb, _ = torch.max(out, axis=1, keepdim=True)\n",
    "out = out - maxb # Keep the values non-positive to avoid blowing up the exponents\n",
    "p = torch.exp(out)\n",
    "p = p / p.sum(axis=1, keepdim=True) # Softmax\n",
    "\n",
    "# Cross entropy loss\n",
    "loss = -torch.mean(torch.log(torch.gather(p, 1, Y.unsqueeze(dim=1)))).item()\n",
    "\n",
    "\n",
    "h1.shape, out.shape, p.shape, loss, flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e46374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "\n",
    "# cross_entropy grad\n",
    "dout = p - F.one_hot(Y)\n",
    "\n",
    "# Output Layer grad\n",
    "dw2 = torch.sum(dout.unsqueeze(dim=2) @ h1.unsqueeze(dim=1), axis=0)\n",
    "db2 = dout.sum(axis=0)\n",
    "dh1 = dout @ w2\n",
    "\n",
    "# Hidden Layer grad\n",
    "dlin1 = torch.zeros_like(dh1)\n",
    "mask = lin1 >= 0\n",
    "dlin1[mask] = dh1[mask]\n",
    "dw1 = torch.sum(dlin1.unsqueeze(dim=2) @ flat.unsqueeze(dim=1), axis=0)\n",
    "db1 = dlin1.sum(axis=0)\n",
    "\n",
    "dout.shape, dw2.shape, db2.shape, dh1.shape, dw1.shape, db1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "w2 = w2 - lr * dw2\n",
    "b2 = b2 - lr * db2\n",
    "w1 = w1 - lr * dw1\n",
    "b1 = b1 - lr * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d56735",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass testing\n",
    "\n",
    "torch.manual_seed(42)\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "outn = net(X)\n",
    "tloss = F.cross_entropy(outn, Y)\n",
    "\n",
    "tloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e427295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self):\n",
    "        d = 28 * 28\n",
    "        d1 = 512\n",
    "        k = 10\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.w1 = torch.empty((d1, d))\n",
    "        self.b1 = torch.empty(d1)\n",
    "        nn.init.kaiming_uniform_(self.w1, a=math.sqrt(5))\n",
    "        bound = 1 / math.sqrt(d)\n",
    "        nn.init.uniform_(self.b1, -bound, bound)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.w2 = torch.empty((k, d1))\n",
    "        self.b2 = torch.empty(k)\n",
    "        nn.init.kaiming_uniform_(self.w2, a=math.sqrt(5))\n",
    "        bound = 1 / math.sqrt(d1)\n",
    "        nn.init.uniform_(self.b2, -bound, bound)\n",
    "        \n",
    "    def backprop(self, X, Y):\n",
    "        ## Forward Pass\n",
    "        \n",
    "        # Hidden Layer\n",
    "        lin1 = flat @ self.w1.T + self.b1\n",
    "        h1 = torch.maximum(lin1, torch.tensor(0)) # ReLU. Let broadcast do its work\n",
    "        \n",
    "        # Output Layer\n",
    "        logits = h1 @ self.w2.T + self.b2\n",
    "        \n",
    "        # Softmax\n",
    "        maxb, _ = torch.max(logits, axis=1, keepdim=True)\n",
    "        logits = logits - maxb # Keep the values non-positive to avoid blowing up the exponents\n",
    "        p = torch.exp(logits)\n",
    "        p = p / p.sum(axis=1, keepdim=True) # Softmax\n",
    "        \n",
    "        # Cross Entropy\n",
    "        loss = -torch.mean(torch.log(torch.gather(p, 1, Y.unsqueeze(dim=1)))).item()\n",
    "        \n",
    "        ## Backward Pass\n",
    "        # cross_entropy grad\n",
    "        dout = p - F.one_hot(Y)\n",
    "\n",
    "        # Output Layer grad\n",
    "        dw2 = torch.sum(dout.unsqueeze(dim=2) @ h1.unsqueeze(dim=1), axis=0)\n",
    "        db2 = dout.sum(axis=0)\n",
    "        dh1 = dout @ self.w2\n",
    "\n",
    "        # Hidden Layer grad\n",
    "        dlin1 = torch.zeros_like(dh1)\n",
    "        mask = lin1 >= 0\n",
    "        dlin1[mask] = dh1[mask]\n",
    "        dw1 = torch.sum(dlin1.unsqueeze(dim=2) @ flat.unsqueeze(dim=1), axis=0)\n",
    "        db1 = dlin1.sum(axis=0)\n",
    "        \n",
    "        # Update\n",
    "        self.w2 = self.w2 - lr * dw2\n",
    "        self.b2 = self.b2 - lr * db2\n",
    "        self.w1 = self.w1 - lr * dw1\n",
    "        self.b1 = self.b1 - lr * db1\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More Testing\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 64\n",
    "X = torch.rand(N, 28, 28)\n",
    "Y = torch.randint(0, k, (N,))\n",
    "lr = 0.001\n",
    "\n",
    "for _ in range(100):\n",
    "    loss = mlp.backprop(X, Y)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "p = np.array([0, 1, 0])\n",
    "y = np.array([0, 5, 2])\n",
    "\n",
    "for _ in range(1):\n",
    "    p = p - 0.02 * 2/3 * x.T @ (x @ p - y)\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [23, 54, -12, 912, 6, 7, 12]\n",
    "mn = np.mean(a)\n",
    "var = np.sum(np.square(a - mn)) / (len(a) - 1)\n",
    "std = np.sqrt(var)\n",
    "\n",
    "std, (a - mn) / 339.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfefbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
